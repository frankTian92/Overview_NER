{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CRFPP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1:获取文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读入CoNLL Corpus\n",
    "def read_CoNLL(filename):\n",
    "    docs = []\n",
    "    word = []\n",
    "    POS = []\n",
    "    chunking = []\n",
    "    NE = []\n",
    "    doc = []\n",
    "    \n",
    "    f = open(filename,encoding = 'gbk')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        line = line.replace('\\n','')\n",
    "        if line == '-DOCSTART- -X- O O':           \n",
    "            if i != 0:\n",
    "                docs.append(doc)\n",
    "                doc = []\n",
    "            doc.append('-DOCSTART-')\n",
    "        else:\n",
    "            if line != '':\n",
    "                doc.append(line.split(' ')[0])\n",
    "            else:\n",
    "                doc.append('_space')\n",
    "        \n",
    "        \n",
    "        if (line != ''):\n",
    "            labels = line.split(' ')\n",
    "            word.append(labels[0])\n",
    "            POS.append(labels[1])\n",
    "            chunking.append(labels[2])\n",
    "            NE.append(labels[3])\n",
    "        else:\n",
    "            word.append('_space')\n",
    "            POS.append('_space')\n",
    "            chunking.append('_space')\n",
    "            NE.append('_space')\n",
    "        \n",
    "        i += 1\n",
    "    docs.append(doc)\n",
    "\n",
    "    df = pd.DataFrame(data = np.transpose(np.array([word,POS,chunking,NE])), columns = ['word','POS','chunking','NE'])\n",
    "    return df, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,docs_train = read_CoNLL('eng.train')\n",
    "df_test,docs_test = read_CoNLL('eng.testa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 常用词列表（至少在五个文档中出现过的词汇）\n",
    "def create_vocab(data):\n",
    "    vocab = {}\n",
    "    if type(data) == pd.core.frame.DataFrame:       \n",
    "        keys = set(data['word'])\n",
    "        for key in keys:\n",
    "            vocab[key] = data['word'].tolist().count(key)\n",
    "    else:\n",
    "        keys = []\n",
    "        for d in data:\n",
    "            if d not in keys:\n",
    "                keys.append(d)\n",
    "        for key in keys:\n",
    "            if type(key) == list:\n",
    "                vocab[[(x,y) for x,y in [key]][0]] = data.count(key)\n",
    "            else:\n",
    "                vocab[key] = data.count(key)\n",
    "    return vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(df_train)\n",
    "\n",
    "common_vocab = []\n",
    "for word in list(vocab.keys()):\n",
    "    n = 0\n",
    "    for doc in docs_train:\n",
    "        if word in doc:\n",
    "            n += 1\n",
    "    \n",
    "    if n >= 5:\n",
    "        common_vocab.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2:抽取基本特征相关的词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 一元相关性模型：（N+ - N-）/(N+ * N-)^0.5\n",
    "def unigram_correlation(df,label):\n",
    "    word_list = [df.loc[i-1][0] for i in range(1,len(df)) if df.loc[i][3] == label]\n",
    "    correlation_list = [[key,(val*2-len(word_list))/(val*(len(word_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(word_list).keys()),list(create_vocab(word_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:20]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_co_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space'):\n",
    "        uni_co_train[label] = unigram_correlation(df_train,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['_space', -1.5737791036443216],\n",
       " ['(', -1.6706290926962695],\n",
       " ['in', -3.3276962395907033],\n",
       " ['of', -4.296491617348425],\n",
       " [',', -4.471963064195109],\n",
       " ['the', -4.627458172020232],\n",
       " ['to', -6.503940668982656],\n",
       " ['and', -6.807502430723574],\n",
       " ['-', -8.310606149789805],\n",
       " ['at', -8.348537942952419],\n",
       " ['AT', -8.845770951417105],\n",
       " ['from', -8.983816697866123],\n",
       " ['New', -9.128204403591983],\n",
       " ['NEW', -10.51263779132837],\n",
       " ['South', -10.587346661068436],\n",
       " ['In', -11.070377493393803],\n",
       " ['--', -11.43033000237937],\n",
       " ['The', -11.525763194434314],\n",
       " ['United', -11.62355454266102],\n",
       " ['Czech', -12.383283438574125]]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_co_train['I-LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 二元相关性模型：\n",
    "def bigram_correlation(df,label):\n",
    "    word_list = [[df.loc[i-2][0],df.loc[i-1][0]] for i in range(2,len(df)) if df.loc[i][3] == label]\n",
    "    correlation_list = [[key,(val*2-len(word_list))/(val*(len(word_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(word_list).keys()),list(create_vocab(word_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_co_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space'):\n",
    "        bi_co_train[label] = bigram_correlation(df_train,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('.', '_space'), -2.260715479119852],\n",
       " [('-DOCSTART-', '_space'), -6.85099052540913],\n",
       " [('in', 'the'), -10.985425548656982],\n",
       " [('_space', 'In'), -11.157266542466408],\n",
       " [('_space', 'NEW'), -11.43033000237937],\n",
       " [('_space', '--'), -11.525763194434314],\n",
       " [('_space', '('), -12.88827288788054],\n",
       " [('of', 'the'), -12.88827288788054],\n",
       " [('the', 'United'), -13.309228000905287],\n",
       " [('_space', 'At'), -13.61333938829869],\n",
       " [('town', 'of'), -14.864377128504293],\n",
       " [('_space', 'The'), -15.28876350919457],\n",
       " [('(', 'South'), -15.514804047460249],\n",
       " [('(', 'Czech'), -15.514804047460249],\n",
       " [('1996-08-28', '_space'), -16.52883936259703],\n",
       " [('at', 'the'), -16.814472022175316],\n",
       " [('CRICKET', '-'), -16.814472022175316],\n",
       " [('city', 'of'), -17.432463832402867],\n",
       " [('_space', '-'), -18.12295328982313],\n",
       " [(')', '_space'), -18.12295328982313]]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_co_train['I-LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 词后缀列表：\n",
    "def NE_suffix(df,label):\n",
    "    suffix_list = [df.loc[i][0][-3:-1]+df.loc[i][0][-1] for i in range(0,len(df)) if df.loc[i][3] == label]\n",
    "    correlation_list = [[key,(val*2-len(suffix_list))/(val*(len(suffix_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(suffix_list).keys()),list(create_vocab(suffix_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space'):\n",
    "        suffix_train[label] = NE_suffix(df_train,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['and', -4.522322603158466],\n",
       " ['.S.', -4.884082377482271],\n",
       " ['ain', -5.529671646651143],\n",
       " ['any', -7.306528827371882],\n",
       " ['nia', -7.525192507858331],\n",
       " ['lia', -7.641615476822893],\n",
       " ['ina', -7.826130242363489],\n",
       " ['sia', -7.826130242363489],\n",
       " ['nce', -7.858117203586373],\n",
       " ['ria', -7.956334249890998],\n",
       " ['ica', -8.199677789140186],\n",
       " ['tan', -8.586928723402199],\n",
       " ['aly', -9.031217387333442],\n",
       " ['New', -9.177828854319243],\n",
       " ['DON', -9.228233860749807],\n",
       " ['ong', -9.228233860749807],\n",
       " ['pan', -9.604783237103666],\n",
       " ['den', -9.84079779868143],\n",
       " ['uth', -9.902455748422403],\n",
       " ['dia', -10.29734931201734],\n",
       " ['ted', -10.43943508230555],\n",
       " ['ium', -10.43943508230555],\n",
       " ['NEW', -10.51263779132837],\n",
       " ['raq', -10.902339336003102],\n",
       " ['nds', -10.902339336003102],\n",
       " ['ORK', -10.985425548656982],\n",
       " ['est', -11.070377493393803],\n",
       " ['nya', -11.070377493393803],\n",
       " ['don', -11.157266542466408],\n",
       " ['ton', -11.157266542466408],\n",
       " ['lic', -11.246167953573321],\n",
       " ['ael', -11.337161146201318],\n",
       " ['tes', -11.826615110746435],\n",
       " ['ech', -11.932101417519752],\n",
       " ['bia', -11.932101417519752],\n",
       " ['TON', -12.75639744407298],\n",
       " ['ine', -13.77324782912327],\n",
       " ['cow', -13.77324782912327],\n",
       " ['zil', -13.77324782912327],\n",
       " ['dan', -14.28825503298024],\n",
       " ['ast', -14.472927008635557],\n",
       " ['ork', -14.664806599621125],\n",
       " ['Sri', -14.664806599621125],\n",
       " ['nka', -14.664806599621125],\n",
       " ['ran', -14.864377128504293],\n",
       " ['ada', -14.864377128504293],\n",
       " ['AND', -15.072168570580073],\n",
       " ['lin', -15.28876350919457],\n",
       " ['kia', -15.514804047460249],\n",
       " ['lem', -15.750999863355617]]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_train['I-LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 命名实体后缀列表：\n",
    "def NE_word_suffix(df,label):\n",
    "    NE_suffix_list = []\n",
    "    new_label = label.split('-')\n",
    "    for i in range(len(df)):        \n",
    "        if (df.loc[i][3] != 'O' and df.loc[i][3] != '_space'):\n",
    "            if (i != len(df)-1):\n",
    "                if (df.loc[i][3].split('-')[1] == new_label[1] and df.loc[i+1][3] != label):\n",
    "                    NE_suffix_list.append(df.loc[i][0])\n",
    "            else:\n",
    "                if (df.loc[i][3].split('-')[1] == new_label[1]):\n",
    "                    NE_suffix_list.append(df.loc[i][0])\n",
    "    \n",
    "    correlation_list = [[key,(val*2-len(NE_suffix_list))/(val*(len(NE_suffix_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(NE_suffix_list).keys()),list(create_vocab(NE_suffix_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_word_suffix_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space' and label != 'B-LOC' and label != 'B-ORG' and label != 'B-PER' and label != 'B-MISC'):\n",
    "        NE_word_suffix_train[label] = NE_word_suffix(df_train,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Clinton', -8.245115032387872],\n",
       " ['Arafat', -11.72315051429123],\n",
       " ['Yeltsin', -11.72315051429123],\n",
       " ['Dole', -12.12461140788624],\n",
       " ['Lebed', -12.415663569902774],\n",
       " ['Ahmed', -12.56906015743073],\n",
       " ['Akram', -13.243235775286516],\n",
       " ['Dutroux', -13.429000507587448],\n",
       " ['Younis', -15.255073960997647],\n",
       " ['Croft', -15.255073960997647],\n",
       " ['Mullally', -15.83820195656401],\n",
       " ['Netanyahu', -16.15556295630695],\n",
       " ['Khan', -16.15556295630695],\n",
       " ['Martin', -16.15556295630695],\n",
       " ['Sohail', -17.233725429434745],\n",
       " ['Johnson', -17.233725429434745],\n",
       " ['Teresa', -17.643317521253234],\n",
       " ['Lewis', -18.083156657703075],\n",
       " ['Medvedev', -18.083156657703075],\n",
       " ['Meri', -18.55716988289701],\n",
       " ['Rubin', -19.070037542225247],\n",
       " ['Christie', -19.070037542225247],\n",
       " ['Anwar', -19.62738989775],\n",
       " ['Cork', -19.62738989775],\n",
       " ['Philippoussis', -19.62738989775],\n",
       " ['Stich', -19.62738989775],\n",
       " ['Chang', -20.236070341914424],\n",
       " ['Sampras', -20.236070341914424],\n",
       " ['Williams', -20.236070341914424],\n",
       " ['Graf', -20.236070341914424],\n",
       " ['Martinez', -20.236070341914424],\n",
       " ['Peres', -20.90449362528667],\n",
       " ['Russell', -20.90449362528667],\n",
       " ['Diana', -20.90449362528667],\n",
       " ['Weizman', -20.90449362528667],\n",
       " ['Morris', -20.90449362528667],\n",
       " ['Gore', -20.90449362528667],\n",
       " ['Jones', -21.643142489760102],\n",
       " ['Mickelson', -21.643142489760102],\n",
       " ['Muster', -21.643142489760102],\n",
       " ['Brady', -21.643142489760102],\n",
       " ['Stewart', -22.465270697649842],\n",
       " ['Thorpe', -22.465270697649842],\n",
       " ['Malik', -22.465270697649842],\n",
       " ['Brown', -22.465270697649842],\n",
       " ['Lauck', -22.465270697649842],\n",
       " ['Levy', -23.387922148241035],\n",
       " ['Rose', -23.387922148241035],\n",
       " ['White', -23.387922148241035],\n",
       " ['Crawley', -23.387922148241035]]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_word_suffix_train['I-PER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 功能词汇（命名实体中出现的小写词汇）\n",
    "def function_word(df,label):\n",
    "    function_word_list = [df.loc[i][0] for i in range(0,len(df)) if (df.loc[i][3] == label and df.loc[i][0].islower() == True)]\n",
    "    correlation_list = [[key,(val*2-len(function_word_list))/(val*(len(function_word_list)-val)+1)**0.5] for key,val in zip(list(create_vocab(function_word_list).keys()),list(create_vocab(function_word_list).values()))]\n",
    "    return sorted(correlation_list,key=operator.itemgetter(1),reverse=True)[:20]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_word_train = {}\n",
    "for label in set(df_train['NE']):\n",
    "    if (label != 'O' and label != '_space' and label != 'B-LOC' and label != 'B-ORG' and label != 'B-PER' and label != 'B-MISC'):\n",
    "        function_word_train[label] = function_word(df_train,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3：局部特征选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS tag\n",
    "tag2index = {}\n",
    "index2tag = {}\n",
    "n = 2\n",
    "tag2index['punc'] = 1\n",
    "index2tag[1] = 'punc'\n",
    "tag2index['unk'] = 0\n",
    "index2tag[0] = 'unk'\n",
    "for item in set(df_train['POS']):\n",
    "    if (item.isupper() == True):\n",
    "        tag2index[item] = n\n",
    "        index2tag[n] = item\n",
    "        n += 1\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagger(df):\n",
    "    tag_list =[]\n",
    "    for tag in df['POS']:\n",
    "        if tag in list(tag2index.keys()):\n",
    "            tag_list.append(tag2index[tag])\n",
    "        else:\n",
    "            if tag.isupper() == False:\n",
    "                tag_list.append(tag2index['punc'])\n",
    "            else:\n",
    "                tag_list.append(tag2index['unk'])\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tag_train = POS_tagger(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tag_test = POS_tagger(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 句首词，大小写，文档的位置（四个参数：首字母大写与否、是不是句首词、全大写与否、文档的位置（头文件:1，标题:2，信息:3，文本:4））\n",
    "def first_capital(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if word[0].isupper():\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_word(df):\n",
    "    tag_list = []\n",
    "    for i in range(len(df)):\n",
    "        if (i>0):\n",
    "            if (df['word'][i-1] == '_space' or df['word'][i-1] == '.' or df['word'][i-1] == '!' or df['word'][i-1] == '?'):\n",
    "                tag_list.append(1)\n",
    "            else:\n",
    "                tag_list.append(0)\n",
    "        else:\n",
    "            tag_list.append(1)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_capitals(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if word.isupper():\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_locations(df,docs):\n",
    "    tag_list = []\n",
    "    docs_lens = []\n",
    "    docs_segments = []\n",
    "    for doc in docs:\n",
    "        split_points = []\n",
    "        i = 0\n",
    "        while (i < 3):\n",
    "            for n in range(len(doc)):\n",
    "                if doc[n] == '_space':\n",
    "                    split_points.append(n)\n",
    "                    i += 1\n",
    "        docs_segments.append(split_points)\n",
    "        docs_lens.append(len(doc))\n",
    "        \n",
    "    \n",
    "    num_words = 0\n",
    "    for j in range(len(docs_lens)):\n",
    "        check_words = df['word'][num_words:num_words+docs_lens[j]]\n",
    "        for cw in range(len(check_words)):\n",
    "            if (cw <= docs_segments[j][0]):\n",
    "                tag_list.append(1)\n",
    "            elif (cw > docs_segments[j][0] and cw <= docs_segments[j][1]):\n",
    "                tag_list.append(2)\n",
    "            elif (cw > docs_segments[j][1] and cw <= docs_segments[j][2]):\n",
    "                tag_list.append(3)\n",
    "            else:\n",
    "                tag_list.append(4)\n",
    "        num_words += docs_lens[j]\n",
    "    \n",
    "    if (len(tag_list) == len(df)):\n",
    "        return tag_list\n",
    "    else:\n",
    "        print('error')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cap_train = first_capital(df_train)\n",
    "first_word_train = first_word(df_train)\n",
    "all_capitals_train = all_capitals(df_train)\n",
    "doc_loc_train = document_locations(df_train,docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cap_test = first_capital(df_test)\n",
    "first_word_test = first_word(df_test)\n",
    "all_capitals_test = all_capitals(df_test)\n",
    "doc_loc_test = document_locations(df_test,docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 语言符号信息 （主要包括. , / % $ -） 以及数字信息\n",
    "def symbal_information(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if ('.' in word or ',' in word or '/' in word or '%' in word or '$' in word or '-' in word):\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_information(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if (re.search('\\d',word) != None):\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbal_infor_train = symbal_information(df_train)\n",
    "number_infor_train = number_information(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbal_infor_test = symbal_information(df_test)\n",
    "number_infor_test = number_information(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 引号，括号信息\n",
    "def filled_information(df):\n",
    "    li_1 = []\n",
    "    li_2 = []\n",
    "    li_3 = []\n",
    "    li_4 = []\n",
    "    li_5 = []\n",
    "    n_1 = 0\n",
    "    n_2 = 0\n",
    "    n_3 = 0\n",
    "    n_4 = 0\n",
    "    n_5 = 0\n",
    "    for word in df['word']:\n",
    "        if (word == '('):\n",
    "            n_1 = 1\n",
    "        if (word == ')'):\n",
    "            n_1 = 0\n",
    "        if (word == '['):\n",
    "            n_2 = 1\n",
    "        if (word == ']'):\n",
    "            n_2 = 0\n",
    "        if (word == '{'):\n",
    "            n_3 = 1\n",
    "        if (word == '}'):\n",
    "            n_3 =0\n",
    "        if (word == \"'\"):\n",
    "            n_4 += 1\n",
    "        if (word == '\"'):\n",
    "            n_5 += 1\n",
    "        li_1.append(n_1)\n",
    "        li_2.append(n_2)\n",
    "        li_3.append(n_3)\n",
    "        li_4.append(n_4%2)\n",
    "        li_5.append(n_5%2)\n",
    "        \n",
    "    tag_list = [int(x) for x in list(np.array(li_1)+np.array(li_2)+np.array(li_3)+np.array(li_4)+np.array(li_5))]\n",
    "    \n",
    "    return tag_list\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_infor_train = filled_information(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_infor_test = filled_information(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 常用词列表\n",
    "def common_word(df):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if word not in common_vocab:\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_word_train =  common_word(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_word_test =  common_word(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 二元特征提取\n",
    "def bi_feature(df,label):\n",
    "    tag_list = []\n",
    "    tag_list.append(0)\n",
    "    tag_list.append(0)\n",
    "    for i in range(2,len(df)):\n",
    "        if (df['NE'][i] != 'O' and df['NE'][i] != '_space'):\n",
    "            if (df['NE'][i].split('-')[1] == label and (df['word'][i-2],df['word'][i-1]) in [x for [x,y] in bi_co_train['I-'+label]]):\n",
    "                tag_list.append(1)\n",
    "            else:\n",
    "                tag_list.append(0)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_LOC_train = bi_feature(df_train,'LOC')\n",
    "bi_PER_train = bi_feature(df_train,'PER')\n",
    "bi_ORG_train = bi_feature(df_train,'ORG')\n",
    "bi_MISC_train = bi_feature(df_train,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_LOC_test = bi_feature(df_test,'LOC')\n",
    "bi_PER_test = bi_feature(df_test,'PER')\n",
    "bi_ORG_test = bi_feature(df_test,'ORG')\n",
    "bi_MISC_test = bi_feature(df_test,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 词后缀特征\n",
    "def suffix_feature(df,label):\n",
    "    tag_list = []\n",
    "    for i in range(len(df)):\n",
    "        if (df['NE'][i] != 'O' and df['NE'][i] != '_space'):\n",
    "            if (df['NE'][i].split('-')[1] == label and df['word'][i][-3:-1]+df['word'][i][-1] in [x for [x,y] in suffix_train['I-'+label]]):\n",
    "                tag_list.append(1)\n",
    "            else:\n",
    "                tag_list.append(0)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_LOC_train = suffix_feature(df_train,'LOC')\n",
    "suffix_PER_train = suffix_feature(df_train,'PER')\n",
    "suffix_ORG_train = suffix_feature(df_train,'ORG')\n",
    "suffix_MISC_train = suffix_feature(df_train,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_LOC_test = suffix_feature(df_test,'LOC')\n",
    "suffix_PER_test = suffix_feature(df_test,'PER')\n",
    "suffix_ORG_test = suffix_feature(df_test,'ORG')\n",
    "suffix_MISC_test = suffix_feature(df_test,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 类别后缀特征(一元特征)\n",
    "def c_suffix_feature(df,label):\n",
    "    tag_list = []\n",
    "    for i in range(len(df)-1):\n",
    "        if (df['NE'][i+1] != 'O' and df['NE'][i+1] != '_space'):\n",
    "            if (df['NE'][i+1].split('-')[1] == label and df['word'][i+1][0].isupper() == True and df['word'][i][0].isupper() == True):\n",
    "                tag_list.append(1)\n",
    "            else:\n",
    "                tag_list.append(0)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "csuffix_LOC_train = c_suffix_feature(df_train,'LOC')\n",
    "csuffix_PER_train = c_suffix_feature(df_train,'PER')\n",
    "csuffix_ORG_train = c_suffix_feature(df_train,'ORG')\n",
    "csuffix_MISC_train = c_suffix_feature(df_train,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "csuffix_LOC_test = c_suffix_feature(df_test,'LOC')\n",
    "csuffix_PER_test = c_suffix_feature(df_test,'PER')\n",
    "csuffix_ORG_test = c_suffix_feature(df_test,'ORG')\n",
    "csuffix_MISC_test = c_suffix_feature(df_test,'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 功能词汇特征\n",
    "def functional_word_train(df,label):\n",
    "    tag_list = []\n",
    "    for word in df['word']:\n",
    "        if word in [x for [x,y] in function_word_train[label]]:\n",
    "            tag_list.append(1)\n",
    "        else:\n",
    "            tag_list.append(0)\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_LOC_train = functional_word_train(df_train,'I-LOC')\n",
    "functional_PER_train = functional_word_train(df_train,'I-PER')\n",
    "functional_ORG_train = functional_word_train(df_train,'I-ORG')\n",
    "functional_MISC_train = functional_word_train(df_train,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_LOC_test = functional_word_train(df_test,'I-LOC')\n",
    "functional_PER_test = functional_word_train(df_test,'I-PER')\n",
    "functional_ORG_test = functional_word_train(df_test,'I-ORG')\n",
    "functional_MISC_test = functional_word_train(df_test,'I-MISC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4：全局特征选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 全局一元特征 \n",
    "def global_uni_feature(docs,label):\n",
    "    tag_list = []\n",
    "    n = 0\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(docs[i])):\n",
    "            inds = [a-1 for a,b in enumerate(docs[i]) if b==docs[i][j]]\n",
    "            inds.remove(j-1)\n",
    "            is_uni = 0\n",
    "            for ind in inds:\n",
    "                if docs[i][ind] in [x for [x,_] in uni_co_train[label]]:\n",
    "                    is_uni = 1\n",
    "            tag_list.append(is_uni)     \n",
    "            n += 1\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "guni_LOC_train = global_uni_feature(docs_train,'I-LOC')\n",
    "guni_PER_train = global_uni_feature(docs_train,'I-PER')\n",
    "guni_ORG_train = global_uni_feature(docs_train,'I-ORG')\n",
    "guni_MISC_train = global_uni_feature(docs_train,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "guni_LOC_test = global_uni_feature(docs_test,'I-LOC')\n",
    "guni_PER_test = global_uni_feature(docs_test,'I-PER')\n",
    "guni_ORG_test = global_uni_feature(docs_test,'I-ORG')\n",
    "guni_MISC_test = global_uni_feature(docs_test,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 全局二元特征 \n",
    "def global_bi_feature(docs,label):\n",
    "    tag_list = []\n",
    "    n = 0\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(docs[i])):\n",
    "            inds = [(a-2,a-1) for a,b in enumerate(docs[i]) if b==docs[i][j]]\n",
    "            inds.remove((j-2,j-1))\n",
    "            is_bi = 0\n",
    "            for ind in inds:\n",
    "                if (docs[i][ind[0]],docs[i][ind[1]]) in [x for [x,_] in bi_co_train[label]]:\n",
    "                    is_bi = 1\n",
    "            tag_list.append(is_bi)     \n",
    "            n += 1\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbi_LOC_train = global_bi_feature(docs_train,'I-LOC')\n",
    "gbi_PER_train = global_bi_feature(docs_train,'I-PER')\n",
    "gbi_ORG_train = global_bi_feature(docs_train,'I-ORG')\n",
    "gbi_MISC_train = global_bi_feature(docs_train,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbi_LOC_test = global_bi_feature(docs_test,'I-LOC')\n",
    "gbi_PER_test = global_bi_feature(docs_test,'I-PER')\n",
    "gbi_ORG_test = global_bi_feature(docs_test,'I-ORG')\n",
    "gbi_MISC_test = global_bi_feature(docs_test,'I-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 全局词后缀特征 \n",
    "def global_csuffix_feature(docs,label,csuffix):\n",
    "    tag_list = []\n",
    "    n = 0\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(docs[i])):\n",
    "            inds = [a-j for a,b in enumerate(docs[i]) if b==docs[i][j]]\n",
    "            inds.remove(0)\n",
    "            is_suf = 0\n",
    "            for ind in inds:\n",
    "                if csuffix[n+ind] == 1:\n",
    "                    is_suf = 1\n",
    "            tag_list.append(is_suf)     \n",
    "            n += 1\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcsuf_LOC_train = global_csuffix_feature(docs_train,'I-LOC',csuffix_LOC_train)\n",
    "gcsuf_PER_train = global_csuffix_feature(docs_train,'I-PER',csuffix_PER_train)\n",
    "gcsuf_ORG_train = global_csuffix_feature(docs_train,'I-ORG',csuffix_ORG_train)\n",
    "gcsuf_MISC_train = global_csuffix_feature(docs_train,'I-MISC',csuffix_MISC_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcsuf_LOC_test = global_csuffix_feature(docs_test,'I-LOC',csuffix_LOC_test)\n",
    "gcsuf_PER_test = global_csuffix_feature(docs_test,'I-PER',csuffix_PER_test)\n",
    "gcsuf_ORG_test = global_csuffix_feature(docs_test,'I-ORG',csuffix_ORG_test)\n",
    "gcsuf_MISC_test = global_csuffix_feature(docs_test,'I-MISC',csuffix_MISC_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step5：训练CRF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 标签\n",
    "NE_list = {}\n",
    "NE_list['O'] = 0\n",
    "NE_list['_space'] = 1\n",
    "NE_list['B-LOC'] = 2\n",
    "NE_list['I-LOC'] = 3\n",
    "NE_list['B-PER'] = 4\n",
    "NE_list['I-PER'] = 5\n",
    "NE_list['B-ORG'] = 6\n",
    "NE_list['I-ORG'] = 7\n",
    "NE_list['B-MISC'] = 8\n",
    "NE_list['I-MISC'] = 9\n",
    "def output():\n",
    "    NE_train = []\n",
    "    NE_test = []\n",
    "    for tag in df_train['NE']:\n",
    "        NE_train.append(NE_list[tag])\n",
    "    for tag_t in df_test['NE']:\n",
    "        NE_test.append(NE_list[tag_t])\n",
    "    return NE_train,NE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = pd.DataFrame(data = np.transpose(np.array([list(df_train['word']),\n",
    "POS_tag_train,\n",
    "symbal_infor_train,\n",
    "number_infor_train,\n",
    "first_cap_train,\n",
    "first_word_train,\n",
    "all_capitals_train,\n",
    "doc_loc_train,\n",
    "fil_infor_train,\n",
    "common_word_train,\n",
    "bi_LOC_train,\n",
    "bi_PER_train, \n",
    "bi_ORG_train, \n",
    "bi_MISC_train,\n",
    "suffix_LOC_train,\n",
    "suffix_PER_train,\n",
    "suffix_ORG_train, \n",
    "suffix_MISC_train,\n",
    "csuffix_LOC_train,\n",
    "csuffix_PER_train,\n",
    "csuffix_ORG_train,\n",
    "csuffix_MISC_train,\n",
    "functional_LOC_train,\n",
    "functional_PER_train,\n",
    "functional_ORG_train,\n",
    "functional_MISC_train,\n",
    "guni_LOC_train,\n",
    "guni_PER_train,\n",
    "guni_ORG_train,\n",
    "guni_MISC_train,\n",
    "gbi_LOC_train,\n",
    "gbi_PER_train,\n",
    "gbi_ORG_train,\n",
    "gbi_MISC_train,\n",
    "gcsuf_LOC_train,\n",
    "gcsuf_PER_train,\n",
    "gcsuf_ORG_train,\n",
    "gcsuf_MISC_train,\n",
    "list(df_train['NE'])])))\n",
    "\n",
    "test_input = pd.DataFrame(data = np.transpose(np.array([list(df_test['word']),\n",
    "POS_tag_test,\n",
    "symbal_infor_test,\n",
    "number_infor_test,\n",
    "first_cap_test,\n",
    "first_word_test,\n",
    "all_capitals_test,\n",
    "doc_loc_test,\n",
    "fil_infor_test,\n",
    "common_word_test,\n",
    "bi_LOC_test,\n",
    "bi_PER_test, \n",
    "bi_ORG_test, \n",
    "bi_MISC_test,\n",
    "suffix_LOC_test,\n",
    "suffix_PER_test,\n",
    "suffix_ORG_test,\n",
    "suffix_MISC_test,\n",
    "csuffix_LOC_test,\n",
    "csuffix_PER_test,\n",
    "csuffix_ORG_test,\n",
    "csuffix_MISC_test,\n",
    "functional_LOC_test,\n",
    "functional_PER_test,\n",
    "functional_ORG_test,\n",
    "functional_MISC_test,\n",
    "guni_LOC_test,\n",
    "guni_PER_test,\n",
    "guni_ORG_test,\n",
    "guni_MISC_test,\n",
    "gbi_LOC_test,\n",
    "gbi_PER_test,\n",
    "gbi_ORG_test,\n",
    "gbi_MISC_test,\n",
    "gcsuf_LOC_test,\n",
    "gcsuf_PER_test,\n",
    "gcsuf_ORG_test,\n",
    "gcsuf_MISC_test,\n",
    "list(df_test['NE'])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = list(np.array(train_input.iloc[:,0]))\n",
    "\n",
    "train_features_o = list(np.array(train_input.iloc[:,1:38]))\n",
    "train_features = []\n",
    "for i in range(len(train_features_o)):\n",
    "    train_f = []\n",
    "    for j in range(len(train_features_o[i])):\n",
    "        if (i == 0):\n",
    "            train_f.append(u'-1:f'+str(j)+u'=0')\n",
    "            train_f.append(u'f'+str(j)+u'='+str(train_features_o[i][j]))\n",
    "            train_f.append(u'+1:f'+str(j)+u'='+str(train_features_o[i+1][j]))\n",
    "        elif (i == len(train_features_o)-1):\n",
    "            train_f.append(u'-1:f'+str(j)+u'='+str(train_features_o[i-1][j]))\n",
    "            train_f.append(u'f'+str(j)+u'='+str(train_features_o[i][j]))\n",
    "            train_f.append(u'+1:f'+str(j)+u'=0')   \n",
    "        else:\n",
    "            train_f.append(u'-1:f'+str(j)+u'='+str(train_features_o[i-1][j]))\n",
    "            train_f.append(u'f'+str(j)+u'='+str(train_features_o[i][j]))\n",
    "            train_f.append(u'+1:f'+str(j)+u'='+str(train_features_o[i+1][j]))  \n",
    "    train_features.append(train_f)\n",
    "\n",
    "train_labels = list(np.array(train_input.iloc[:,38]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = list(np.array(test_input.iloc[:,0]))\n",
    "\n",
    "test_features_o = list(np.array(test_input.iloc[:,1:38]))\n",
    "test_features = []\n",
    "for i in range(len(test_features_o)):\n",
    "    test_f = []\n",
    "    for j in range(len(test_features_o[i])):\n",
    "        if (i == 0):\n",
    "            test_f.append(u'-1:f'+str(j)+u'=0')\n",
    "            test_f.append(u'f'+str(j)+u'='+str(test_features_o[i][j]))\n",
    "            test_f.append(u'+1:f'+str(j)+u'='+str(test_features_o[i+1][j]))\n",
    "        elif (i == len(test_features_o)-1):\n",
    "            test_f.append(u'-1:f'+str(j)+u'='+str(test_features_o[i-1][j]))\n",
    "            test_f.append(u'f'+str(j)+u'='+str(test_features_o[i][j]))\n",
    "            test_f.append(u'+1:f'+str(j)+u'=0')   \n",
    "        else:\n",
    "            test_f.append(u'-1:f'+str(j)+u'='+str(test_features_o[i-1][j]))\n",
    "            test_f.append(u'f'+str(j)+u'='+str(test_features_o[i][j]))\n",
    "            test_f.append(u'+1:f'+str(j)+u'='+str(test_features_o[i+1][j]))  \n",
    "    test_features.append(test_f)\n",
    "\n",
    "test_labels = list(np.array(test_input.iloc[:,38]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(words,features,labels):\n",
    "    x = []\n",
    "    y = []\n",
    "    x_sents = []\n",
    "    y_sents = []\n",
    "    for i in range(len(words)):\n",
    "        if (words[i] == '_space'):\n",
    "            x_sents.append([words[i]]+list(features[i]))\n",
    "            y_sents.append(labels[i])               \n",
    "            x.append(x_sents)\n",
    "            y.append(y_sents)\n",
    "            x_sents = []\n",
    "            y_sents = []            \n",
    "        else:\n",
    "            x_sents.append([words[i]]+list(features[i]))\n",
    "            y_sents.append(labels[i])\n",
    "    x.append(x_sents)\n",
    "    y.append(y_sents)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = get_input(train_words,train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = get_input(test_words,test_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 训练CRF模型\n",
    "trainer = pycrfsuite.Trainer(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xseq, yseq in zip(x_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.8 s, sys: 234 ms, total: 54.1 s\n",
      "Wall time: 54.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('CRF.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'active_features': 3359,\n",
       " 'error_norm': 1502.594098,\n",
       " 'feature_norm': 78.716283,\n",
       " 'linesearch_step': 1.0,\n",
       " 'linesearch_trials': 1,\n",
       " 'loss': 8484.519646,\n",
       " 'num': 50,\n",
       " 'scores': {},\n",
       " 'time': 0.814}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step6:测试CRF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x1a38c92da0>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('CRF.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: O O O O O O O O O I-PER I-PER O I-MISC O O I-PER I-PER O O O O O _space\n",
      "Correct:   O O O O O O O O O I-PER I-PER O I-MISC O O I-PER I-PER O O O O O _space\n"
     ]
    }
   ],
   "source": [
    "example_sent = x_test[15]\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(example_sent)))\n",
    "print(\"Correct:  \", ' '.join(y_test[15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRFsuite的使用方法:需要对每个句子进行输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'} - {'_space'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 s, sys: 6.76 ms, total: 2.05 s\n",
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      I-LOC       0.94      0.94      0.94      2094\n",
      "     B-MISC       0.00      0.00      0.00         4\n",
      "     I-MISC       0.98      0.90      0.94      1264\n",
      "      I-ORG       0.96      0.92      0.94      2092\n",
      "      I-PER       0.97      0.97      0.97      3149\n",
      "\n",
      "avg / total       0.96      0.94      0.95      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bio_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
